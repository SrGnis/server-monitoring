///////////////////////////////////////////////////////////////////////////////
// Configuration file
local.file "endpoints" {
    // The endpoints file is used to define the endpoints, credentials and options
    // for the Alloy export to.
    filename = "/etc/alloy/endpoints.json"
}

///////////////////////////////////////////////////////////////////////////////
// Metrics scraping

// Scrape Tempo, Mimir, Phlare and Loki
// We use the prometheus.scrape component and give this a unique label.
prometheus.scrape "monitor_infra" {
    // The targets array allows us to specify which service targets to scrape from.
    // Define the address to scrape from, and add a 'group' and 'service' label for each target.
    targets = [
        {"__address__" = "localhost:9009", group = "infrastructure", service = "mimir"},
        {"__address__" = "localhost:3100", group = "infrastructure", service = "loki"},
        {"__address__" = "localhost:3000", group = "infrastructure", service = "grafana"},
    ]

    // Scrape all of these services every 15 seconds.
    scrape_interval = "15s"
    // Send the metrics to the prometheus remote write receiver for exporting to Mimir.
    forward_to = [prometheus.remote_write.mimir.receiver]
    // The job name to add to the scraped metrics.
    job_name = "monitor_infra"
}

// Scrape the local Alloy itself.
prometheus.scrape "alloy" {
    // Only one target, the Alloy, it's part of the 'infrastructure' group.
    targets = [{"__address__" = "localhost:12345", group = "infrastructure", service = "alloy"}]
    // Send the metrics to the prometheus remote write receiver for exporting to Mimir.
    forward_to = [prometheus.remote_write.mimir.receiver]
    // Attach job name to the metrics.
    job_name = "alloy"
}

// This block relabels metrics coming from node_exporter to add standard labels
discovery.relabel "integrations_node_exporter" {
    targets = prometheus.exporter.unix.integrations_node_exporter.targets

    rule {
        // Set the instance label to the hostname of the machine
        target_label = "instance"
        replacement  = constants.hostname
    }

    rule {
        // Set a standard job name for all node_exporter metrics
        target_label = "job"
        replacement = "integrations/node_exporter"
    }
}

// Configure the node_exporter integration to collect system metrics
prometheus.exporter.unix "integrations_node_exporter" {
    // Disable unnecessary collectors to reduce overhead
    disable_collectors = ["ipvs", "btrfs", "infiniband", "xfs", "zfs"]
    enable_collectors = ["meminfo"]

    filesystem {
        // Exclude filesystem types that aren't relevant for monitoring
        fs_types_exclude     = "^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|tmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$"
        // Exclude mount points that aren't relevant for monitoring
        mount_points_exclude = "^/(dev|proc|run/credentials/.+|sys|var/lib/docker/.+)($|/)"
        // Timeout for filesystem operations
        mount_timeout        = "5s"
    }

    netclass {
        // Ignore virtual and container network interfaces
        ignored_devices = "^(br-.*|veth.*|cali.*|[a-f0-9]{15})$"
    }

    netdev {
        // Exclude virtual and container network interfaces from device metrics
        device_exclude = "^(br-.*|veth.*|cali.*|[a-f0-9]{15})$"
    }
}

// Define how to scrape metrics from the node_exporter
prometheus.scrape "integrations_node_exporter" {
    scrape_interval = "15s"
    // Use the targets with labels from the discovery.relabel component
    targets    = discovery.relabel.integrations_node_exporter.output
    // Send the scraped metrics to the relabeling component
    forward_to = [prometheus.remote_write.mimir.receiver]
}

// The prometheus.remote_write component defines an endpoint for remotely writing metrics to.
// In this case, our locally running Mimir service.
prometheus.remote_write "mimir" {
    // The endpoint is the Mimir service.
    endpoint {
        url = json_path(local.file.endpoints.content, ".metrics.url")[0]

        // Basic auth credentials. If the endpoint is not TLS, whilst sent, these will be ignored.
        basic_auth {
            username = json_path(local.file.endpoints.content, ".metrics.basicAuth.username")[0]
            password = json_path(local.file.endpoints.content, ".metrics.basicAuth.password")[0]
        }
    }
}

///////////////////////////////////////////////////////////////////////////////
// Logging

// Collect logs from systemd journal for node_exporter integration
loki.source.journal "logs_integrations_integrations_node_exporter_journal_scrape" {
    // Only collect logs from the last 24 hours
    max_age       = "24h0m0s"
    // Apply relabeling rules to the logs
    relabel_rules = discovery.relabel.logs_integrations_integrations_node_exporter_journal_scrape.rules
    // Send logs to the local Loki instance
    forward_to    = [loki.write.local.receiver]
}

// Define which log files to collect for node_exporter
local.file_match "logs_integrations_integrations_node_exporter_direct_scrape" {
    path_targets = [{
        // Target localhost for log collection
        __address__ = "localhost",
        // Collect standard system logs
        __path__    = "/var/log/{syslog,messages,*.log}",
        // Add instance label with hostname
        instance    = constants.hostname,
        // Add job label for logs
        job         = "integrations/node_exporter",
    }]
}

// Define relabeling rules for systemd journal logs
discovery.relabel "logs_integrations_integrations_node_exporter_journal_scrape" {
    targets = []

    rule {
        // Extract systemd unit information into a label
        source_labels = ["__journal__systemd_unit"]
        target_label  = "unit"
    }

    rule {
        // Extract boot ID information into a label
        source_labels = ["__journal__boot_id"]
        target_label  = "boot_id"
    }

    rule {
        // Extract transport information into a label
        source_labels = ["__journal__transport"]
        target_label  = "transport"
    }

    rule {
        // Extract log priority into a level label
        source_labels = ["__journal_priority_keyword"]
        target_label  = "level"
    }
}

// Collect logs from files for node_exporter
loki.source.file "logs_integrations_integrations_node_exporter_direct_scrape" {
    // Use targets defined in local.file_match
    targets    = local.file_match.logs_integrations_integrations_node_exporter_direct_scrape.targets
    // Send logs to the local Loki instance
    forward_to = [loki.write.local.receiver]
}

loki.write "local" {
    endpoint {
            url = json_path(local.file.endpoints.content, ".logs.url")[0]

            // Basic auth credentials. If the endpoint is not TLS, whilst sent, these will be ignored.
            basic_auth {
                username = json_path(local.file.endpoints.content, ".logs.basicAuth.username")[0]
                password = json_path(local.file.endpoints.content, ".logs.basicAuth.password")[0]
            }
        }
}